{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e683b94",
   "metadata": {},
   "source": [
    "PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e14027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef9b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Successfully loaded 'dataset.csv'\n",
      "Preprocessing...\n",
      "Grouping diagnosis codes into categories...\n",
      "Creating total_visits and med_changes features...\n",
      "Splitting data...\n",
      "Starting Hyperparameter Tuning (this may take 5-10 minutes)...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 148\u001b[0m\n\u001b[0;32m    138\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    139\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf,\n\u001b[0;32m    140\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Prints progress\u001b[39;00m\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Run the search\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[0;32m    151\u001b[0m model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    971\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    972\u001b[0m         clone(base_estimator),\n\u001b[0;32m    973\u001b[0m         X,\n\u001b[0;32m    974\u001b[0m         y,\n\u001b[0;32m    975\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    976\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    977\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    978\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    979\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    981\u001b[0m     )\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    983\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    985\u001b[0m     )\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\utente\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CELL: Full Pipeline - ADVANCED (GridSearchCV + Feature Engineering + SHAP)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier # Using RandomForest\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Explicitly import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading data...\")\n",
    "file_path = 'dataset.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded '{file_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{file_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the file: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Preprocessing ---\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "# Drop index, encounter_id, and patient_id (not predictive)\n",
    "df = df.drop(['index', 'encounter_id', 'patient_id'], axis=1)\n",
    "\n",
    "# Handle weight (unchanged)\n",
    "df['weight'] = df['weight'].replace('?', np.nan)\n",
    "df['weight'] = df['weight'].str.replace('[', '').str.replace(')', '').str.split('-').str[0]\n",
    "df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "\n",
    "# Handle age (unchanged)\n",
    "age_map = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35, '[40-50)': 45,\n",
    "    '[50-60)': 55, '[60-70)': 65, '[70-80)': 75, '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "df['age'] = df['age'].map(age_map)\n",
    "\n",
    "# --- NEW: ADVANCED FEATURE ENGINEERING (Diagnosis Grouping) ---\n",
    "print(\"Grouping diagnosis codes into categories...\")\n",
    "# We will group diag_1, diag_2, and diag_3\n",
    "diag_cols = ['diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "# First, convert codes to numeric (as before)\n",
    "for col in diag_cols:\n",
    "    df[col] = pd.to_numeric(df[col].astype(str).str.extract('(\\d+)')[0], errors='coerce')\n",
    "\n",
    "# Define the ICD-9 categories\n",
    "def group_diagnosis(icd9_code):\n",
    "    if pd.isna(icd9_code):\n",
    "        return 'Missing'\n",
    "    icd9_code = float(icd9_code)\n",
    "    if (icd9_code >= 390 and icd9_code <= 459) or icd9_code == 785:\n",
    "        return 'Circulatory'\n",
    "    if (icd9_code >= 460 and icd9_code <= 519) or icd9_code == 786:\n",
    "        return 'Respiratory'\n",
    "    if (icd9_code >= 520 and icd9_code <= 579) or icd9_code == 787:\n",
    "        return 'Digestive'\n",
    "    if icd9_code == 250:\n",
    "        return 'Diabetes'\n",
    "    if icd9_code >= 800 and icd9_code <= 999:\n",
    "        return 'Injury'\n",
    "    if icd9_code >= 710 and icd9_code <= 739:\n",
    "        return 'Musculoskeletal'\n",
    "    if icd9_code >= 580 and icd9_code <= 629 or icd9_code == 788:\n",
    "        return 'Genitourinary'\n",
    "    if icd9_code >= 140 and icd9_code <= 239:\n",
    "        return 'Neoplasms'\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the grouping\n",
    "for col in diag_cols:\n",
    "    df[col] = df[col].apply(group_diagnosis)\n",
    "    \n",
    "# We now have categorical diagnosis codes instead of ~800 numbers.\n",
    "# We also drop diag_4 and diag_5 as they are less important and for simplicity\n",
    "df = df.drop(['diag_4', 'diag_5'], axis=1)\n",
    "# --- END NEW FEATURE ENGINEERING ---\n",
    "\n",
    "# Handle X1-X25 medications (unchanged)\n",
    "med_cols = [f'X{i}' for i in range(1, 26)]\n",
    "for col in ['X1', 'X2']:\n",
    "    df[col] = df[col].replace('None', 0)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "change_map = {'No': 0, 'Up': 1, 'Down': -1, 'Steady': 0}\n",
    "for col in med_cols[2:]:\n",
    "    df[col] = df[col].map(change_map).fillna(0)\n",
    "\n",
    "# Categorical encoding\n",
    "# Add our new diag_cols to the list\n",
    "cat_cols = ['race', 'gender', 'medical_specialty', 'change', 'diabetesMed', 'diag_1', 'diag_2', 'diag_3']\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna('missing')\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Fill remaining NaNs\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Feature Engineering (from last time)\n",
    "print(\"Creating total_visits and med_changes features...\")\n",
    "df['total_visits'] = df['number_inpatient'] + df['number_emergency'] + df['number_outpatient']\n",
    "med_change_cols = [f'X{i}' for i in range(3, 26)] # X3 to X25\n",
    "df['med_changes'] = (df[med_change_cols] != 0).sum(axis=1) # Count any 'Up' or 'Down'\n",
    "\n",
    "# Target\n",
    "y = df['readmitted']\n",
    "X = df.drop('readmitted', axis=1)\n",
    "\n",
    "# --- 3. Train-Test Split ---\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- 4. Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"Starting Hyperparameter Tuning (this may take 5-10 minutes)...\")\n",
    "\n",
    "# Define the model we want to tune\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "# NOTE: This is a small grid to save time. Research papers use much larger grids.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # Number of trees\n",
    "    'max_depth': [10, 20, 30]         # Max depth of each tree\n",
    "}\n",
    "\n",
    "# Set up the GridSearch\n",
    "# We are optimizing for 'f1_weighted' to match the research papers\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3, # 3-fold cross-validation\n",
    "    scoring='f1_weighted', # This is the score we want to maximize\n",
    "    n_jobs=-1, # Use all available cores\n",
    "    verbose=2  # Prints progress\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best f1_weighted score from tuning: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nFinal Test Set Results:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Readmitted (0)', 'Readmitted (1)']))\n",
    "\n",
    "\n",
    "# --- 5. SHAP Explainability (Modern API) ---\n",
    "# (This section is unchanged, but will now use the *tuned* model)\n",
    "print(\"Computing SHAP values...\")\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "explanation_object = explainer(X_test)\n",
    "\n",
    "\n",
    "# --- 6. SHAP Plots (Separated for Readability) ---\n",
    "# (This section is unchanged)\n",
    "print(\"\\n--- Generating SHAP Plots (One by One) ---\")\n",
    "explanation_class_1 = explanation_object[:, :, 1]\n",
    "\n",
    "print(\"Displaying Interactive Force Plot (for first prediction, Class 1)...\")\n",
    "display(shap.force_plot(explanation_object[0, :, 1]))\n",
    "\n",
    "print(\"\\nGenerating Waterfall Plot...\")\n",
    "plt.figure()\n",
    "shap.waterfall_plot(explanation_class_1[0], max_display=10, show=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating Feature Importance (Bar Plot)...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(explanation_class_1, plot_type=\"bar\", max_display=15, show=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating Beeswarm Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(explanation_class_1, max_display=15, show=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating Dependence Plots...\")\n",
    "shap_values_class_1 = explanation_class_1.values\n",
    "feature_names = explanation_class_1.feature_names\n",
    "top_features_indices = np.argsort(np.abs(shap_values_class_1).mean(0))[-3:][::-1]\n",
    "\n",
    "for i, idx in enumerate(top_features_indices):\n",
    "    plt.figure()\n",
    "    print(f\"Plotting dependence for: {feature_names[idx]}\")\n",
    "    shap.dependence_plot(\n",
    "        feature_names[idx], \n",
    "        shap_values_class_1, \n",
    "        X_test,\n",
    "        show=False\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 7. Print Top 10 Important Features ---\n",
    "# (This section is unchanged)\n",
    "print(\"\\nTop 10 Most Important Features (by mean |SHAP|):\")\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': np.abs(shap_values_class_1).mean(0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(shap_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nAll SHAP plots generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
